{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e3f81e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\merav\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x29db584cc90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nbimporter\n",
    "import dataset_class as StreetSign\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "from sklearn.metrics import accuracy_score\n",
    "from os.path import join\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from os import path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1328)\n",
    "torch.random.manual_seed(1328);\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c8e1cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize normale\n",
    "tran = transforms.Compose([StreetSign.Rescale(256),StreetSign.RandomCrop(32),StreetSign.ToTensor()])\n",
    "#tran = None\n",
    "\n",
    "datatrain = StreetSign.StreetSignDataset('DITS-full\\DITS-full\\DITS-detection\\class\\classes.csv','DITS-full\\DITS-full\\DITS-detection\\class\\image',tran)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0c8ce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize normale\n",
    "datatest = StreetSign.StreetSignDataset('DITS-full\\DITS-full\\DITS-detection\\detection_test\\day\\classes.csv','DITS-full\\DITS-full\\DITS-detection\\detection_test\\day',tran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "624fa233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MiniAlexNet(nn.Module):\n",
    "    def __init__(self, input_channels=3, out_classes=100):\n",
    "        super(MiniAlexNet, self).__init__()\n",
    "        \n",
    "        # Ridefiniamo il modello utilizzando i moduli sequential.\n",
    "        # Ne definiamo due: un \"feature extractor\" che estrae le feature maps\n",
    "        # e un \"classificatore\" che implementa i livelli fully connected.\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            # Conv1\n",
    "            nn.Conv2d(input_channels, 16, 5, padding=2),\n",
    "            # Input: 3 x 32 x 32. Output: 16 x 32 x 32\n",
    "            nn.MaxPool2d(2),\n",
    "            # Input: 16 x 32 x 32. Output: 16 x 16 x 16\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Conv2\n",
    "            nn.Conv2d(16, 32, 5, padding=2),\n",
    "            # Input: 16 x 16 x 16. Output: 32 x 16 x 16\n",
    "            nn.MaxPool2d(2),\n",
    "            # Input: 32 x 16 x 16. Output: 32 x 8 x 8\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Conv3\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            # Input: 32 x 8 x 8. Output: 64 x 8 x 8\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Conv4\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            # Input: 64 x 8 x 8. Output: 128 x 8 x 8\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Conv5\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            # Input: 128 x 8 x 8. Output: 256 x 8 x 8\n",
    "            nn.MaxPool2d(2),\n",
    "            # Input: 256 x 8 x 8. Output: 256 x 4 x 4\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            # FC6\n",
    "            nn.Linear(4096, 2048),  # Input: 256 * 4 * 4\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # FC7\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # FC8\n",
    "            nn.Linear(1024, out_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Applichiamo le diverse trasformazioni in cascata\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.classifier(x.view(x.shape[0], -1))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0acc31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funzioni\n",
    "\n",
    "# train:\n",
    "from os.path import join\n",
    "import torch.optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_classifier(model, train_loader, test_loader, exp_name='segnali', lr = 0.001,\n",
    "                     epochs= 10, momentum = 0.9, logdir='logs'):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr, momentum=momentum)\n",
    "    \n",
    "    # meters:\n",
    "    loss_meter = AverageValueMeter()\n",
    "    acc_meter = AverageValueMeter()\n",
    "    \n",
    "    writer= SummaryWriter(join(logdir, exp_name))\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    \n",
    "    # definiamo un dizionario con i loader di train e test:\n",
    "    loader = {\n",
    "        'train' : train_loader,\n",
    "        'test' : test_loader\n",
    "        }\n",
    "    # inizializzo il global step:\n",
    "    global_step = 0\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for mode in ['train', 'test']:\n",
    "            loss_meter.reset(); acc_meter.reset()\n",
    "            \n",
    "            model.train() if mode == 'train' else model.eval()\n",
    "            \n",
    "            with torch.set_grad_enabled(mode == 'train'):\n",
    "                for i, batch in enumerate(loader[mode]):\n",
    "                    #print(f\"Exemplo de dados de treinamento {i}:\")\n",
    "                    #print(batch)\n",
    "                    x = batch['image'].to(device).float()\n",
    "                    y = batch['landmarks'].to(device).long()\n",
    "                    \n",
    "                    # Ridimensiona y in un tensore 1D\n",
    "                    y = y.view(-1)\n",
    "                    \n",
    "                    output = model(x)\n",
    "                    # aggiorniamo il global_step che conta il numero di campioni visti durante il training:\n",
    "                    n = x.shape[0]  # n elementi nel batch\n",
    "                    global_step += n\n",
    "                    l = criterion(output, y)\n",
    "                    \n",
    "                    if mode == 'train':\n",
    "                        l.backward()\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad\n",
    "                    \n",
    "                    acc = accuracy_score(y.to('cpu'), output.to('cpu').max(1)[1])\n",
    "                    loss_meter.add(l.item(), n)\n",
    "                    acc_meter.add(acc, n)\n",
    "                    \n",
    "                    # loggiamo i dati iterazione per iterazione solo durante il training:\n",
    "                    if mode == 'train':\n",
    "                        writer.add_scalar('loss/train', loss_meter.value(), global_step=global_step)\n",
    "                        writer.add_scalar('accuracy/train', acc_meter.value(), global_step=global_step)\n",
    "                        \n",
    "            # loggiamo le stime finali sia di training che di test alla fine dell'epoca\n",
    "            writer.add_scalar('loss/' + mode, loss_meter.value(), global_step=global_step)\n",
    "            writer.add_scalar('accuracy/' + mode, acc_meter.value(), global_step=global_step)\n",
    "        \n",
    "        # conserviamo i pesi del modello alla fine di un ciclo di traininge test:\n",
    "        torch.save(model.state_dict(), \"%s-%d.pth\" % (exp_name, e+1))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18ff5c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predizioni sul test set\n",
    "def test_classifier(model, loader):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    predictions, labels = [], []\n",
    "    for batch in loader:\n",
    "        x = batch['image'].to(device).float()\n",
    "        y = batch['landmarks'].to(device).long()\n",
    "        \n",
    "        output = model(x)\n",
    "        preds = output.to(\"cpu\").max(1)[1].numpy()\n",
    "        labs = y.to(\"cpu\").numpy()\n",
    "        \n",
    "        predictions.extend(list(preds))\n",
    "        labels.extend(list(labs))\n",
    "        \n",
    "    return np.array(predictions), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6a8312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valutazione Modello:\n",
    "\n",
    "# Average Value Meter: servir√† per calcolare la media delle loss pesata sulle dim dei batch\n",
    "class AverageValueMeter():\n",
    "    def __init(self):\n",
    "        super(AverageValueMeter,self).__init__()\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.num = 0\n",
    "    \n",
    "    def add(self, value, num):\n",
    "        self.sum += num * value\n",
    "        self.num += num\n",
    "    \n",
    "    def value(self):\n",
    "        try:\n",
    "            return self.sum/self.num\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9acd476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REC Curve\n",
    "\n",
    "def rec_curve(predictions, gt):\n",
    "    assert predictions.shape == gt.shape\n",
    "    # calcoliamo tutti gli errori mediante MAE\n",
    "    errors = np.abs(np.array((predictions-gt)))\n",
    "    \n",
    "    # prendiamo i valori unici degli errori e ordiniamoli\n",
    "    tolerances = sorted(np.unique(errors))\n",
    "    correct= [] #lista delle \"accuracy\" relative a ogni soglia\n",
    "    \n",
    "    for t in tolerances:\n",
    "        correct.append((errors<=t).mean()) # frazione di elementi \"correttamente\" regressi\n",
    "    AUC = np.trapz(correct, tolerances) #a rea sotto la curva calcolata col metodo dei trapezi\n",
    "    tot_area = np.max(tolerances)*1 # area totale\n",
    "    AOC = tot_area - AUC\n",
    "    # restituiamo le soglie, la frazione di campioni correttamente regressi e l'area sopra la curva\n",
    "    return tolerances, correct, AOC\n",
    "\n",
    "# Creiamo il grafico delle curve\n",
    "#plt.figure(figsize=(8, 6))\n",
    "#plt.plot(tolerances, correct, marker='o', linestyle='-')\n",
    "#plt.xlabel('Tolleranza')\n",
    "#plt.ylabel('Accuratezza')\n",
    "#plt.title('Curva REC (Relative Error Curve)')\n",
    "#plt.grid(True)\n",
    "\n",
    "# Mostra il grafico a schermo\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4df2a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datatrain_loader = DataLoader(datatrain, batch_size=1024, shuffle=True)\n",
    "\n",
    "datatest_loader = DataLoader(datatest, batch_size=1024, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3233d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniAlexNet(\n",
      "  (feature_extractor): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): ReLU()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=1024, out_features=100, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mini_datatrain = MiniAlexNet()\n",
    "print(mini_datatrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "916f4f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_datatrain = train_classifier(\n",
    "    model=mini_datatrain,\n",
    "    train_loader=datatrain_loader,  # DataLoader di addestramento\n",
    "    test_loader=datatest_loader,    # DataLoader di test o validazione\n",
    "    exp_name='MiniAlexNet_dataset',\n",
    "    epochs=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8482d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LeNetColor su StreetSign: 0.58\n"
     ]
    }
   ],
   "source": [
    "mini_data_test_predictions, data_labels_test = test_classifier(mini_datatrain,\n",
    "                                                                datatest_loader)\n",
    "print(\"Accuracy LeNetColor su StreetSign: %0.2f\" % \\\n",
    "      accuracy_score(data_labels_test,mini_data_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d17fa9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "\n",
    "# %tensorboard --logdir MachineLearning_Unict\\logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
