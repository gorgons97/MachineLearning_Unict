{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c80a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "import dataset_class as StreetSign\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "from sklearn.metrics import accuracy_score\n",
    "from os.path import join\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from os import path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1328)\n",
    "torch.random.manual_seed(1328);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49e9b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize normale\n",
    "tran = transforms.Compose([StreetSign.Rescale(256),StreetSign.RandomCrop(32),StreetSign.ToTensor()])\n",
    "#tran = None\n",
    "\n",
    "datatrain = StreetSign.StreetSignDataset('DITS-full\\DITS-full\\DITS-detection\\class\\classes.csv','DITS-full\\DITS-full\\DITS-detection\\class\\image',tran)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0568145a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "#stamp try\n",
    "\n",
    "sample = datatrain[230]\n",
    "print(sample['image'].size())\n",
    "print(sample['landmarks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e51a7b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize normale\n",
    "datatest = StreetSign.StreetSignDataset('DITS-full\\DITS-full\\DITS-detection\\detection_test\\day\\classes.csv','DITS-full\\DITS-full\\DITS-detection\\detection_test\\day',tran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0cdd9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "#stamp try\n",
    "\n",
    "sample = datatest[10]\n",
    "print(sample['image'].size())\n",
    "print(sample['landmarks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87081d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageValueMeter():\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.num = 0\n",
    "        \n",
    "    def add(self, value, num):\n",
    "        self.sum += value*num\n",
    "        self.num += num\n",
    "        \n",
    "    def value(self):\n",
    "        try :\n",
    "            return self.sum/ self.num\n",
    "        except :\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84e19ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, train_loader, test_loader, exp_name='experiment' ,\n",
    "                     lr=0.01, epochs=100, momentum=0.99, logdir='logs'):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr, momentum=momentum)\n",
    "    #meters\n",
    "    loss_meter = AverageValueMeter()\n",
    "    acc_meter = AverageValueMeter()\n",
    "    #writer\n",
    "    writer = SummaryWriter(join(logdir, exp_name))\n",
    "    #device\n",
    "    device = \"cpu\" #\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    #definiamo un dizionario contenente i Loader di training e test\n",
    "    loader = {\n",
    "        'train' : train_loader,\n",
    "        'test' : test_loader\n",
    "    }\n",
    "    #iniziaLizziamo iL global step\n",
    "    global_step = 0\n",
    "    for e in range(epochs):\n",
    "        #iteriamo tra due modalità: train e test\n",
    "        for mode in ['train' , 'test']:\n",
    "            loss_meter.reset(); acc_meter.reset()\n",
    "            model.train() if mode == 'train' else model.eval()\n",
    "            with torch.set_grad_enabled(mode=='train'): #abiLitiamo i gradienti SOLO in training\n",
    "                for i, batch in enumerate(loader[mode]):\n",
    "                    x = list(batch.values())[0].to(device).float()\n",
    "                    y = list(batch.values())[1].to(device).long()  # Assicurati che y sia di tipo Long\n",
    "                    y = y.squeeze()\n",
    "\n",
    "                    #print(y)  # Stampa il tensore target y per il controllo\n",
    "                    output = model(x)\n",
    "                    \n",
    "                    #aggiorniamo iL gLobaL_step\n",
    "                    #conterrà iL numero di campioni visti durante iL training\n",
    "                    n = x.shape[0] #numero di elementi nel batch\n",
    "                    global_step += n\n",
    "                    l = criterion(output, y)\n",
    "                    \n",
    "                    if mode=='train' :\n",
    "                        l.backward()\n",
    "                        optimizer.step( )\n",
    "                        optimizer.zero_grad ( )\n",
    "                    \n",
    "                    acc = accuracy_score(y.to('cpu'),output.to('cpu').max(1)[1])\n",
    "                    loss_meter.add(l.item(), n)\n",
    "                    acc_meter.add(acc,n)\n",
    "                    \n",
    "                    #Loggiamo i risultati iterazione per iterazione SOLO durante iL training\n",
    "                    if mode=='train' :\n",
    "                        writer.add_scalar( ' loss/train ' ,loss_meter.value(), global_step=global_step)\n",
    "                        writer.add_scalar( 'accuracy/train' ,acc_meter.value(), global_step=global_step)            \n",
    "                #una voLta finita L 'epoca (sia nel caso di training che test, Loggiamo Le stime finali)\n",
    "                writer.add_scalar( 'loss/' + mode, loss_meter.value(), global_step=global_step)\n",
    "                writer.add_scalar( 'accuracy/' + mode, acc_meter.value(), global_step=global_step)\n",
    "        #conserviamo i pesi del model Lo aLLa fine di un ciclo di training e test\n",
    "        torch.save(model.state_dict(), 'modelli\\%s-%d.pth'%(exp_name,e+1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c06a8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(model, loader):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    predictions, labels = [], []\n",
    "    for batch in loader:\n",
    "        x = list(batch.values())[0].to(device).float()\n",
    "        y = list(batch.values())[1].to(device).long()  \n",
    "        y = y.squeeze()\n",
    "        \n",
    "        output = model(x)\n",
    "        preds = output.to('cpu').max(1)[1].numpy()\n",
    "        labs = y.to('cpu').numpy()\n",
    "        predictions.extend(list(preds))\n",
    "        labels.extend(list(labs))\n",
    "    return np.array(predictions), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c4dec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetColor(nn.Module):\n",
    "    def __init__ (self):\n",
    "        super(LeNetColor, self).__init__()\n",
    "        #ridefiniamo iL modeLLo utilizzando i moduli sequentiaL.\n",
    "        #ne definiamo due: un \"feature extractor\", che estrae Le feature maps\n",
    "        #e un \"classificatore\" che implementa i LiveLLy FC\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 18, kernel_size=5), #Input: 3 x 32 x 32. Ouput: 18 x 28 x 28\n",
    "            nn.MaxPool2d(2), #Input: 18 x 28 x 28. Output: 18 x 14 x 14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(18, 28, 5), #lnput 18 x 14 x 14. Output: 28 x le x le\n",
    "            nn.MaxPool2d(2), #Input 28 x le x le. Output: 28 x 5 x 5\n",
    "            nn.ReLU()\n",
    "        )    \n",
    "            \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(700, 360), #rnput: 28 * 5 * 5\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(360, 252),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(252, 100)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #AppLichiamo Le diverse trasformazioni in cascata\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.classifier(x.view(x.shape[0],-1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56e05b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "datatrain_loader = DataLoader(datatrain, batch_size=1024, shuffle=True)\n",
    "#cifar100_test_loader = DataLoader(cifar100_test, batch_size=1024, num_workers=2)\n",
    "datatest_loader = DataLoader(datatest, batch_size=1024, shuffle=True)\n",
    "#cifar100_test_loader = DataLoader(cifar100_test, batch_size=1024, num_workers=2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a28d396",
   "metadata": {},
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(datatrain_loader))\n",
    "print(f\"Feature batch shape: {train_features}\")\n",
    "print(f\"Labels batch shape: {train_labels}\")\n",
    "img = train_features[0]\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e27cb5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_datatrain = LeNetColor()\n",
    "lenet_datatrain = train_classifier(lenet_datatrain, datatrain_loader, datatest_loader, \\\n",
    "                                 'lenet_dataset', epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98fccb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LeNetColor su StreetSign: 0.58\n"
     ]
    }
   ],
   "source": [
    "lenet_data_test_predictions, data_labels_test = test_classifier(lenet_datatrain,\n",
    "                                                                datatest_loader)\n",
    "print(\"Accuracy LeNetColor su StreetSign: %0.2f\" % \\\n",
    "      accuracy_score(data_labels_test,lenet_data_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9b05be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNetColor(\n",
       "  (feature_extractor): Sequential(\n",
       "    (0): Conv2d(3, 18, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(18, 28, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=700, out_features=360, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=360, out_features=252, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=252, out_features=100, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenet_datatrain.load_state_dict(torch.load('lenet_dataset-1.pth'))\n",
    "lenet_datatrain.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "090408d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniAlexNet(nn.Module):\n",
    "    def __init__(self, input_channels=3, out_classes=100):\n",
    "        super(MiniAlexNet, self).__init__()\n",
    "        #ridefiniamo il modello utilizzando i moduli sequential.\n",
    "        #ne definiamo due: un \"feature extractor\", che estrae le feature map.\n",
    "        #e un \"classificatore\" che implementa i livelly FC\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            #Conv1\n",
    "            nn.Conv2d(input_channels, 16, 5, padding=2), #Input: 3 x 32 x 32. Ouput: 16 x 32 x 32\n",
    "            nn.MaxPool2d(2), #Input: 16 x 32 x 32. Output: 16 x 16 x 16\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Conv2\n",
    "            nn.Conv2d(16, 32, 5, padding=2), #Input 16 x 16 x 16. Output: 32 x 16 x 16\n",
    "            nn.MaxPool2d(2), #Input: 32 x 16 x 16. Output: 32 x 8 x 8\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Conv3\n",
    "            nn.Conv2d(32, 64, 3, padding=1), #Input 32 x 8 x 8. Output: 64 x 8 x 8\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Conv4\n",
    "            nn.Conv2d(64, 128, 3, padding=1), #Input 64 x 8 x 8. Output: 128 x 8 x 8\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Conv5\n",
    "            nn.Conv2d(128, 256, 3, padding=1), #Input 128 x 8 x 8. Output: 256 x 8 x 8\n",
    "            nn.MaxPool2d(2), #Input: 256 x 8 x 8. Output: 256 x 4 x 4\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            #FC6\n",
    "            nn.Linear(4096, 2048), #Input: 256 * 4 * 4\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #FC7\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #FC8\n",
    "            nn.Linear(1024, out_classes)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        #Applichiamo le diverse trasformazioni in cascata\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.classifier(x.view(x.shape[0],-1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21661e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
