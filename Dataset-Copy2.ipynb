{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c80a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "import dataset_class as StreetSign\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "from sklearn.metrics import accuracy_score\n",
    "from os.path import join\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from os import path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1328)\n",
    "torch.random.manual_seed(1328);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49e9b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize normale\n",
    "tran = transforms.Compose([StreetSign.Rescale(256),StreetSign.RandomCrop(32),StreetSign.ToTensor()])\n",
    "#tran = None\n",
    "\n",
    "datatrain = StreetSign.StreetSignDataset('DITS-full\\DITS-full\\DITS-detection\\class\\classes.csv','DITS-full\\DITS-full\\DITS-detection\\class\\image',tran)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0568145a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "#stamp try\n",
    "\n",
    "sample = datatrain[230]\n",
    "print(sample['image'].size())\n",
    "print(sample['landmarks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e51a7b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize normale\n",
    "datatest = StreetSign.StreetSignDataset('DITS-full\\DITS-full\\DITS-detection\\detection_test\\day\\classes.csv','DITS-full\\DITS-full\\DITS-detection\\detection_test\\day',tran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0cdd9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "#stamp try\n",
    "\n",
    "sample = datatest[10]\n",
    "print(sample['image'].size())\n",
    "print(sample['landmarks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87081d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageValueMeter():\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.num = 0\n",
    "        \n",
    "    def add(self, value, num):\n",
    "        self.sum += value*num\n",
    "        self.num += num\n",
    "        \n",
    "    def value(self):\n",
    "        try :\n",
    "            return self.sum/ self.num\n",
    "        except :\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84e19ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, train_loader, test_loader, exp_name='experiment' ,\n",
    "                     lr=0.01, epochs=10, momentum=0.99, logdir='logs'):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr, momentum=momentum)\n",
    "    #meters\n",
    "    loss_meter = AverageValueMeter()\n",
    "    acc_meter = AverageValueMeter()\n",
    "    #writer\n",
    "    writer = SummaryWriter(join(logdir, exp_name))\n",
    "    #device\n",
    "    device = \"cpu\" #\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    #definiamo un dizionario contenente i Loader di training e test\n",
    "    loader = {\n",
    "        'train' : train_loader,\n",
    "        'test' : test_loader\n",
    "    }\n",
    "    #iniziaLizziamo iL global step\n",
    "    global_step = 0\n",
    "    for e in range(epochs):\n",
    "        #iteriamo tra due modalità: train e test\n",
    "        for mode in ['train' , 'test']:\n",
    "            loss_meter.reset(); acc_meter.reset()\n",
    "            model.train() if mode == 'train' else model.eval()\n",
    "            with torch.set_grad_enabled(mode=='train'): #abiLitiamo i gradienti SOLO in training\n",
    "                for i, batch in enumerate(loader[mode]):\n",
    "                    x = list(batch.values())[0].to(device).float()\n",
    "                    y = list(batch.values())[1].to(device).long()  # Assicurati che y sia di tipo Long\n",
    "                    y = y.squeeze()\n",
    "\n",
    "                    #print(y)  # Stampa il tensore target y per il controllo\n",
    "                    output = model(x)\n",
    "                    \n",
    "                    #aggiorniamo iL gLobaL_step\n",
    "                    #conterrà iL numero di campioni visti durante iL training\n",
    "                    n = x.shape[0] #numero di elementi nel batch\n",
    "                    global_step += n\n",
    "                    l = criterion(output, y)\n",
    "                    \n",
    "                    if mode=='train' :\n",
    "                        l.backward()\n",
    "                        optimizer.step( )\n",
    "                        optimizer.zero_grad ( )\n",
    "                    \n",
    "                    acc = accuracy_score(y.to('cpu'),output.to('cpu').max(1)[1])\n",
    "                    loss_meter.add(l.item(), n)\n",
    "                    acc_meter.add(acc,n)\n",
    "                    \n",
    "                    #Loggiamo i risultati iterazione per iterazione SOLO durante iL training\n",
    "                    if mode=='train' :\n",
    "                        writer.add_scalar( ' loss/train ' ,loss_meter.value(), global_step=global_step)\n",
    "                        writer.add_scalar( 'accuracy/train' ,acc_meter.value(), global_step=global_step)            \n",
    "                #una voLta finita L 'epoca (sia nel caso di training che test, Loggiamo Le stime finali)\n",
    "                writer.add_scalar( 'loss/' + mode, loss_meter.value(), global_step=global_step)\n",
    "                writer.add_scalar( 'accuracy/' + mode, acc_meter.value(), global_step=global_step)\n",
    "        #conserviamo i pesi del model Lo aLLa fine di un ciclo di training e test\n",
    "        torch. save(model.state_dict(), '%s-%d.pth'%(exp_name,e+1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c06a8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(model, loader):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    predictions, labels = [], []\n",
    "    for batch in loader:\n",
    "        x = list(batch.values())[0].to(device).float()\n",
    "        y = list(batch.values())[1].to(device).long()  \n",
    "        y = y.squeeze()\n",
    "        \n",
    "        output = model(x)\n",
    "        preds = output.to('cpu').max(1)[1].numpy()\n",
    "        labs = y.to('cpu').numpy()\n",
    "        predictions.extend(list(preds))\n",
    "        labels.extend(list(labs))\n",
    "    return np.array(predictions), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c4dec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MiniAlexNetV2(nn.Module):\n",
    "    def __init__(self, input_channels=3, out_classes=100):\n",
    "        super(MiniAlexNetV2, self).__init__()\n",
    "        \n",
    "        # Definiamo il modello utilizzando moduli Sequential.\n",
    "        # Abbiamo due parti principali: \"feature extractor\" e \"classificatore\".\n",
    "        \n",
    "        # Feature Extractor\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            # Conv1\n",
    "            nn.Conv2d(input_channels, 16, 5, padding=2),\n",
    "            # Input: 3 x 32 x 32. Output: 16 x 32 x 32\n",
    "            nn.MaxPool2d(2),\n",
    "            # Input: 16 x 32 x 32. Output: 16 x 16 x 16\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Conv2\n",
    "            nn.Conv2d(16, 32, 5, padding=2),\n",
    "            # Input: 16 x 16 x 16. Output: 32 x 16 x 16\n",
    "            nn.MaxPool2d(2),\n",
    "            # Input: 32 x 16 x 16. Output: 32 x 8 x 8\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Conv3\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            # Input: 32 x 8 x 8. Output: 64 x 8 x 8\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Conv4\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            # Input: 64 x 8 x 8. Output: 128 x 8 x 8\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Conv5\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            # Input: 128 x 8 x 8. Output: 256 x 8 x 8\n",
    "            nn.MaxPool2d(2),\n",
    "            # Input: 256 x 8 x 8. Output: 256 x 4 x 4\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Classificatore\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            # I layer di dropout vanno posizionati prima di FC6 e FC7\n",
    "            \n",
    "            # FC6\n",
    "            nn.Linear(4096, 2048),\n",
    "            # Input: 256 * 4 * 4\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            \n",
    "            # FC7\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # FC8\n",
    "            nn.Linear(1024, out_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Applichiamo le diverse trasformazioni in cascata\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.classifier(x.view(x.shape[0], -1))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56e05b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "datatrain_loader = DataLoader(datatrain, batch_size=1024, shuffle=True)\n",
    "#cifar100_test_loader = DataLoader(cifar100_test, batch_size=1024, num_workers=2)\n",
    "datatest_loader = DataLoader(datatest, batch_size=1024, shuffle=True)\n",
    "#cifar100_test_loader = DataLoader(cifar100_test, batch_size=1024, num_workers=2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a28d396",
   "metadata": {},
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(datatrain_loader))\n",
    "print(f\"Feature batch shape: {train_features}\")\n",
    "print(f\"Labels batch shape: {train_labels}\")\n",
    "img = train_features[0]\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e27cb5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_datatrain = MiniAlexNetV2()\n",
    "lenet_datatrain = train_classifier(lenet_datatrain, datatrain_loader, datatest_loader, \\\n",
    "                                 'lenet_datatrain', epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98fccb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy MiniAlexNetV2 0.58\n"
     ]
    }
   ],
   "source": [
    "lenet_data_test_predictions, data_labels_test = test_classifier(lenet_datatrain,\n",
    "                                                                datatest_loader)\n",
    "print(\"Accuracy MiniAlexNetV2 %0.2f\" % \\\n",
    "      accuracy_score(data_labels_test,lenet_data_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280cc994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
